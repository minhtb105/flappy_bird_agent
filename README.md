# ğŸ¦ Flappy Bird AI â€“ Reinforcement Learning Agent

ğŸš€ **Flappy Bird AI** is a deep reinforcement learning (RL) agent trained using **Motion Transformers** and **Prioritized Experience Replay (PER)**. The AI learns to **play Flappy Bird autonomously**, adapting to dynamic environments with **stochastic pipes**.

![Flappy Bird AI Demo](assets/demo.gif)  

---

## ğŸš€ Features
- ğŸ§  **Dueling Motion Transformers** â€“ Reduces overestimation of Q-values for stable learning.
- ğŸ¯ **Prioritized Experience Replay (PER)** â€“ Learns faster by focusing on high-error transitions.
- ğŸ“¡ **LIDAR-based Observations** â€“ Bird sees the world via 180-degree raycasting for better perception.
- ğŸŒªï¸ **Stochastic Pipe Generation** â€“ Forces the agent to generalize by varying obstacle positions.
- ğŸ§± **Temporal State Stacking** â€“ Uses `FRAME_STACK=12` to encode time into input.
- ğŸ’¾ **Model Checkpointing** â€“ Saves model every N episodes for resumable training or evaluation.
- ğŸ“ˆ **TensorBoard Logging** â€“ Visualize loss, rewards, Q-values, TD errors and more.
---

## ğŸ“¥ Install Dependencies
Ensure you have Python 3.8+ installed, then run:
```bash
git clone https://github.com/minhtb105/flappy-bird-agent.git
cd flappy-bird-agent
---

# ğŸ¤– Training the AI

Train the **Flappy Bird agent** using Motion Transformers:

```bash
pip install -r requirements.txt
```

Run the training script:

```bash
python train.py
```

- The AI will start learning from scratch!  
- Model checkpoints are saved every 1000 episodes.  
- Training metrics are logged in plots/ for TensorBoard.
---

Launch TensorBoard:

```bash
tensorboard --logdir=plots
```

# ğŸ”¬ How the AI Works

Flappy Bird AI is trained using Dueling Motion Transformers with several optimizations:

### ğŸ§  Neural Network Architecture
- **Input**: The LIDAR sensor 180.
- **Hidden Layers**: Fully connected deep neural network.
- **Output**: Q-values for jump or no jump decisions.

### ğŸ” Replay Buffer Design (Prioritized + Filtered)
The replay buffer is designed for efficiency and focus by selectively retaining high-quality transitions. It includes:

ğŸ§  Filtering Strategy
Before saving the buffer to disk, we filter transitions from memory using the following weighted sampling criteria:

Type	Description	Ratio
ğŸ”„ random_ratio	Uniformly random transitions (ensure diversity)	20%
ğŸ’° reward_ratio	Transitions with high absolute reward values	40%
ğŸ”¥ priority_ratio	Transitions with high TD-error priorities (PER)	40%

This ensures the agent learns from:

both mistakes and successes (reward_ratio),

surprising/difficult transitions (priority_ratio),

and a small set of random noise for generalization (random_ratio).

These filtered samples are saved periodically to disk.

ğŸ’¾ Efficient Batch-Saving & Loading
Instead of saving the entire replay buffer at once (which is large), the filtered samples are:

âœ… Split into batches (e.g., 50,000 samples per chunk)

âœ… Saved as .pt files (e.g., replay_buffer_epXXX_chunk0.pt)

âœ… Reloaded later and merged during fine-tuning or evaluation

This helps reduce memory usage and improves I/O performance during long training runs.

---

ğŸ—‚ Directory Structure
```bash
ğŸ“¦ your-project/
â”œâ”€â”€ train.py # Main training loop
â”œâ”€â”€ agent.py # DQN agent logic
â”œâ”€â”€ game.py # Flappy Bird environment
â”œâ”€â”€ replay_buffer.py # Replay buffer with PER
â”‚ â”œâ”€â”€ configs/
â”‚ â”œâ”€â”€ dqn_configs.py # DQN settings
â”‚ â””â”€â”€ game_configs.py # Game settings
â”‚ â”œâ”€â”€ models/ # Saved models & checkpoints
â”œâ”€â”€ plots/ # Reward, Q-value, loss, TD-error, ... graphs
â”œâ”€â”€ logs/ # logs 
```

# ğŸš€ Future Improvements

- ğŸ”¹ **NeuroEvolution** â€“ Train the AI using genetic algorithms instead of backpropagation.
- ğŸ”¹ **Self-Play** â€“ Train the AI by competing against itself.
- ğŸ”¹ **Multi-Agent Training** â€“ Create multiple birds learning in parallel.
- ğŸ”¹ **Imitation Learning** â€“ Train the AI using human gameplay data.
