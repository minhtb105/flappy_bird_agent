# ğŸ¦ Flappy Bird AI â€“ Reinforcement Learning Agent

ğŸš€ **Flappy Bird AI** is a deep reinforcement learning (RL) agent trained using **Double Deep Q-Networks (DDQN)** and **Prioritized Experience Replay (PER)**. The AI learns to **play Flappy Bird autonomously**, adapting to dynamic environments with **stochastic pipes**.

![Flappy Bird AI Demo](assets/demo.gif)  

---

## ğŸš€ Features
- **ğŸ§  Double Deep Q-Networks (DDQN)** â€“ More stable training by reducing Q-value overestimation.
- **ğŸ¯ Prioritized Experience Replay (PER)** â€“ AI learns faster by focusing on important mistakes.
- **ğŸŒ Stochastic Pipes (Randomized Levels)** â€“ Prevents overfitting by forcing AI to adapt.
- **ğŸ’¾ Model Checkpointing** â€“ Saves progress every 10000 steps for later training or testing.

---

## ğŸ“¥ Install Dependencies
Ensure you have Python 3.8+ installed, then run:
```bash
pip install -r requirements.txt

---

# ğŸ¤– Training the AI

Train the **Flappy Bird agent** using Deep Q-Learning:

```bash
pip install -r requirements.txt
```

Run the training script:

```bash
python train.py
```

The AI will start learning from scratch!  
Model checkpoints are saved every 500 episodes.  
Training graphs are automatically generated to visualize progress.

---

# ğŸ”¬ How the AI Works

Flappy Bird AI is trained using Deep Q-Learning (DQN) with several optimizations:

### ğŸ§  Neural Network Architecture
- **Input**: Bird's `y-position`, `velocity`, `distance to next pipe`, and `gap center`.
- **Hidden Layers**: Fully connected deep neural network.
- **Output**: Q-values for jump or no jump decisions.

### ğŸ—ï¸ Reinforcement Learning Enhancements

| **Feature**                     | **Purpose**                                         |
|----------------------------------|----------------------------------------------------|
| **Double DQN (DDQN)**            | Prevents Q-value overestimation                    |
| **Prioritized Experience Replay (PER)** | Speeds up learning by focusing on important experiences |
| **Stochastic Pipes**             | Forces AI to adapt to random environments          |
| **Soft & Hard Target Network Updates** | Improves stability during training              |

---

# ğŸš€ Future Improvements

- ğŸ”¹ **NeuroEvolution** â€“ Train the AI using genetic algorithms instead of backpropagation.
- ğŸ”¹ **Self-Play** â€“ Train the AI by competing against itself.
- ğŸ”¹ **Multi-Agent Training** â€“ Create multiple birds learning in parallel.
- ğŸ”¹ **Imitation Learning** â€“ Train the AI using human gameplay data.
